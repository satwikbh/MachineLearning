import pickle as pi
import time

import numpy as np
import os
import os.path
import scipy
from sklearn.decomposition import IncrementalPCA
from sklearn.decomposition import PCA as sklearnPCA

from Clustering.DBScanClustering import DBScan
from Utils.LoggerUtil import LoggerUtil


class SVD:
    def __init__(self):
        self.log = LoggerUtil(self.__class__.__name__).get()

    @staticmethod
    def perform_pca(input_matrix, threshold_point):
        """
        Performs PCA by reducing matrix to a dimension where 90% of the data is captured (till the threshold point).
        :param input_matrix:
        :param threshold_point:
        :return:
        """
        sklearn_pca = sklearnPCA(n_components=threshold_point)
        reduced_matrix = sklearn_pca.fit_transform(input_matrix)
        return reduced_matrix

    def perform_incremental_pca(self, input_matrix, threshold_point):
        """
        An incremental version where the partial_fit method is called many times.
        This will provide a different slice of the dataset each time.
        Then performs PCA by reducing matrix to a dimension where 90% of the data is captured (till the threshold point)
        :param input_matrix:
        :param threshold_point:
        :return:
        """
        self.log.info("Entering the {} class".format(SVD.perform_incremental_pca.__name__))

        ipca = IncrementalPCA(n_components=threshold_point)
        ipca.partial_fit(input_matrix)
        reduced_matrix = ipca.transform(input_matrix)
        pi.dump(reduced_matrix, open("Reduced_matrix"))

        self.log.info("Exiting the {} class".format(SVD.perform_incremental_pca.__name__))
        return reduced_matrix

    def get_threshold_point(self, sigma):
        """
        Takes the Eigen Values and then computes the position where 90% of the data is captured.
        :param sigma:
        :return:
        """
        threshold_point = 0
        threshold = sum(sigma) * 0.9
        for x in xrange(len(sigma)):
            if sum(sigma[:x]) > threshold:
                threshold_point = x
                break
        self.log.info("The threshold point is : {}".format(threshold_point))
        return threshold_point

    def singular_value_decomposition(self, input_matrix):
        """
        Performs Singular Value Decomposition and return U, Sigma and VT matrices.
        :param input_matrix:
        :return:
        """
        self.log.info("Entering the {} class".format(SVD.singular_value_decomposition.__name__))

        U, SIGMA, VT = scipy.linalg.svd(input_matrix)
        pi.dump(U, open("U_matrix", "w"))
        pi.dump(SIGMA, open("Sigma_matrix", "w"))
        pi.dump(VT, open("VT_matrix", "w"))

        self.log.info("Exiting the {} class".format(SVD.singular_value_decomposition.__name__))
        return U, SIGMA, VT

    def load_data(self, input_path):
        """
        This will load the data from mycsvfile.csv and convert it into Numpy Array.
        :return:
        """
        input_matrix = None
        names_list = None
        input_matrix_path = input_path + "/" + "input_matrix"
        names_list_path = input_path + "/" + "names_list"
        feature_vector_path = input_path + "/" + "mycsvfile.csv"
        if os.path.exists(input_matrix_path):
            try:
                input_matrix = pi.load(open(input_matrix_path))
            except Exception as e:
                self.log.error("No Such file input_matrix \n Error is : {}".format(e))
        if os.path.exists(names_list_path):
            try:
                names_list = pi.load(open(names_list_path))
            except Exception as e:
                self.log.error("No Such file names_list \n Error is : {}".format(e))
        else:
            try:
                f = open(feature_vector_path)
                l = list(list())
                names = list()

                for lines in f.readlines():
                    split = lines.split(",")
                    names.append(split[0])
                    l.append(list(split[1][:-2]))

                input_matrix = np.array(l)
                names_list = np.array(names)
                pi.dump(input_matrix, open("input_matrix", "w"))
                pi.dump(names_list, open("names_list", "w"))
            except Exception as e:
                self.log.error("Major Exception. Cannot load the input_files \n Error is : {}".format(e))
        return input_matrix, names_list

    def main(self, input_path):
        start_time = time.time()
        self.log.info("Entering the {} class".format(SVD.main.__name__))

        input_matrix, names_list = self.load_data(input_path)
        self.log.info("Matrix Shape {} ".format(input_matrix.shape))

        U, Sigma, VT = self.singular_value_decomposition(input_matrix)

        threshold_point = self.get_threshold_point(Sigma)
        reduced_matrix = U[:, :threshold_point] * Sigma[:threshold_point]

        # reduced_matrix = self.dimensionality_reduction(input_matrix, threshold_point, input_function)

        dbscan = DBScan()
        no_of_clusters = dbscan.dbscan_cluster(reduced_matrix, threshold=0.5)
        self.log.info("The number of clusters is : {}".format(len(no_of_clusters.keys())))

        self.log.info("Exiting the {} class".format(SVD.main.__name__))
        self.log.info("Total time taken : {} ".format(time.time() - start_time))


if __name__ == '__main__':
    svd_object = SVD()
    print("Select the Dimensionality Reduction Technique \n"
          "1. PCA \n"
          "2. Kernel PCA \n"
          "3. LLE \n"
          "4. t-SNE \n"
          "5. Auto Encoders \n")
    n = int(raw_input())
    print("Enter the input_matrix's path: \n")
    input_path = str(raw_input())
    svd_object.main(input_path)
