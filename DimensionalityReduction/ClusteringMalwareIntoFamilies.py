import json
import logging.config
import os
import os.path
import pickle as pi
import time

import numpy as np
from scipy.linalg import svd
from sklearn.cluster import DBSCAN
from sklearn.decomposition import IncrementalPCA
from sklearn.decomposition import PCA as sklearnPCA


class SVD:
    logger = logging.getLogger(__name__)

    def __init__(self):
        self.clustering = Clustering

    @staticmethod
    def setup_logging(default_path='logging.json', default_level=logging.INFO, env_key='LOG_CFG'):
        """
        Setup logging configuration
        :param default_path:
        :param default_level:
        :param env_key:
        :return:
        """
        path = default_path
        value = os.getenv(env_key, None)
        if value:
            path = value
        if os.path.exists(path):
            with open(path, 'rt') as f:
                config = json.load(f)
            logging.config.dictConfig(config)
        else:
            logging.basicConfig(level=default_level)

    @staticmethod
    def perform_pca(input_matrix, threshold_point):
        """
        Performs the PCA by reducing the matrix to a dimension where 90% of the data is captured i.e till the threshold point.
        :param input_matrix:
        :param threshold_point:
        :return:
        """
        sklearn_pca = sklearnPCA(n_components=threshold_point)
        reduced_matrix = sklearn_pca.fit_transform(input_matrix)
        return reduced_matrix

    @staticmethod
    def perform_incremental_pca(input_matrix, threshold_point):
        """
        This is an incremental version where we call the partial_fit method many times, providing a different slice of the dataset each time.
        Performs the PCA by reducing the matrix to a dimension where 90% of the data is captured i.e till the threshold point.
        :param input_matrix:
        :param threshold_point:
        :return:
        """
        SVD.logger.info("Entering the {} class".format(
            SVD.perform_incremental_pca.__name__))
        ipca = IncrementalPCA(n_components=threshold_point)
        ipca.partial_fit(input_matrix)
        reduced_matrix = ipca.transform(input_matrix)
        pi.dump(reduced_matrix, open("Reduced_matrix"))
        SVD.logger.info("Exiting the {} class".format(
            SVD.perform_incremental_pca.__name__))
        return reduced_matrix

    @staticmethod
    def get_threshold_point(sigma):
        """
        Takes the Eigen Values and then computes the position where 90% of the data is captured.
        :param sigma:
        :return:
        """
        threshold_point = 0
        threshold = sum(sigma) * 0.9
        for x in xrange(len(sigma)):
            if sum(sigma[:x]) > threshold:
                threshold_point = x
                break
        SVD.logger.info("The threshold point is : {}".format(threshold_point))
        return threshold_point

    @staticmethod
    def singular_value_decomposition(input_matrix):
        """
        Performs Singular Value Decomposition and return U, Sigma and VT matrices.
        :param input_matrix:
        :return:
        """
        SVD.logger.info("Entering the {} class".format(
            SVD.singular_value_decomposition.__name__))
        U, SIGMA, VT = svd(input_matrix)
        pi.dump(U, open("U_matrix", "w"))
        pi.dump(SIGMA, open("Sigma_matrix", "w"))
        pi.dump(VT, open("VT_matrix", "w"))
        SVD.logger.info("Exiting the {} class".format(
            SVD.singular_value_decomposition.__name__))
        return U, SIGMA, VT

    @staticmethod
    def load_data():
        """
        This will load the data from mycsvfile.csv and convert it into Numpy Array.
        :return:
        """
        input_matrix = None
        names_list = None
        if os.path.exists("input_matrix"):
            try:
                input_matrix = pi.load(open("input_matrix"))
            except Exception as e:
                SVD.logger.error(
                    "No Such file input_matrix \n Error is : {}".format(e))
        if os.path.exists("names_list"):
            try:
                names_list = pi.load(open("names_list"))
            except Exception as e:
                SVD.logger.error(
                    "No Such file names_list \n Error is : {}".format(e))
        else:
            try:
                f = open("mycsvfile.csv")
                l = list(list())
                names = list()

                for lines in f.readlines():
                    split = lines.split(",")
                    names.append(split[0])
                    l.append(list(split[1][:-2]))

                input_matrix = np.array(l)
                names_list = np.array(names)
                pi.dump(input_matrix, open("input_matrix", "w"))
            except Exception as e:
                SVD.logger.error(
                    "Major Exception. Cannot load the input_files \n Error is : {}".format(e))
        return input_matrix, names_list

    def dimensionality_reduction(self, input_matrix, threshold_point, input_function):
        """
        This method takes as input an input matrix and the threshold point and returns the
        reduced_matrix after performing the desired reduction technique.
        :param input_matrix:
        :param threshold_point:
        :param input_function:
        :return:
        """
        switcher = {
            0: self.perform_incremental_pca(input_matrix, threshold_point),
            # FIXME : Should we use pca or incremental pca. This needs to be checked.
            # self.perform_incremental_pca(input_matrix, threshold_point)
            # self.perform_pca(input_matrix, threshold_point)
            1: self.perform_kernal_pca(input_matrix, threshold_point),
            2: self.perform_lle(input_matrix, threshold_point),
            3: self.perform_tsne(input_matrix, threshold_point),
            4: self.perform_sae(input_matrix, threshold_point)
        }
        reduced_matrix = switcher.get(input_function)
        return reduced_matrix

    def main(self, input_function):
        """
        The main function.
        :param input_function:
        :return:
        """
        start_time = time.time()
        SVD.logger.info("Entering the {} class".format(SVD.main.__name__))
        input_matrix, names_list = self.load_data()
        SVD.logger.info("Matrix Shape {} ".format(input_matrix.shape))

        U, Sigma, VT = self.singular_value_decomposition(input_matrix)

        threshold_point = self.get_threshold_point(Sigma)
        reduced_matrix = U[:, :threshold_point] * Sigma[:threshold_point]

        # reduced_matrix = self.dimensionality_reduction(
        #     input_matrix, threshold_point, input_function)

        self.clustering.perform_dbscan(reduced_matrix)

        SVD.logger.info("Exiting the {} class".format(SVD.main.__name__))
        SVD.logger.info("Total time taken : {} ".format(
            time.time() - start_time))


class Clustering:
    def __init__(self):
        pass

    @staticmethod
    def perform_dbscan(reduced_matrix):
        """
        Perform DBScan with the default parameters and return the predicted number of clusters.
        :param reduced_matrix:
        :return:
        """
        dbscan = DBSCAN().fit(reduced_matrix)
        no_of_clusters = dbscan.labels_
        SVD.logger.info(
            "The number of clusters is : {}".format(no_of_clusters))
        return no_of_clusters


if __name__ == '__main__':
    svd_object = SVD()
    svd_object.setup_logging()
    print("Select the Dimensionality Reduction Technique \n"
          "1. PCA \n"
          "2. Kernel PCA \n"
          "3. LLE \n"
          "4. t-SNE \n"
          "5. Auto Encoders \n")
    n = int(raw_input())
    svd_object.main(n)
